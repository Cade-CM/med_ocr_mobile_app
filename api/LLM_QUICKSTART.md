# Quick Start: Add LLM-Enhanced Parsing

Follow these steps to enable local LLM parsing for better OCR accuracy:

## 1. Install Ollama (5 minutes)

**Download and install:**
- Go to: https://ollama.ai/download
- Download Windows installer
- Run the installer
- Ollama will start automatically as a background service

**Verify it's working:**
```powershell
ollama --version
```

## 2. Download the Model (2-5 minutes)

```powershell
ollama pull llama3.2:1b
```

This downloads a 1GB model optimized for speed.

## 3. Install Python Package

```powershell
cd c:\Users\cadem\Desktop\med_ocr_expo\api
pip install ollama
```

## 4. Restart Your Flask API

Stop your current Flask server (Ctrl+C), then:

```powershell
python app.py
```

You should see:
```
âœ“ Ollama available with llama3.2:1b - LLM parsing enabled
 * Running on http://192.168.1.79:5000
```

## Done! ðŸŽ‰

The app will now automatically:
- Use fast regex parsing for clean OCR (most cases)
- Fall back to LLM when confidence < 70%
- Handle OCR errors much better

## Test It

Take a photo of a prescription label. If the OCR has errors, you'll see:
```
Low confidence (65.0%), attempting LLM enhancement...
LLM parsing successful
```

## Performance

- **Before**: Fails on OCR errors like "VERY" instead of "EVERY"
- **After**: LLM corrects errors and extracts accurate data
- **Speed**: Only ~200-500ms slower when LLM is needed
- **Cost**: $0 - completely free and local

## Optional: Use Your 30 Images to Improve Results

If you want even better accuracy:

1. Create a folder for your images:
```powershell
cd c:\Users\cadem\Desktop\med_ocr_expo\api
mkdir training_data
```

2. Copy your 30 prescription images to `training_data/`

3. Run the data collection script:
```powershell
python create_training_data.py
```

4. Label each image interactively

5. Generate few-shot examples:
```powershell
python create_training_data.py few-shot
```

6. Add the examples to `llm_parser.py` prompt (optional - for maximum accuracy)

## Troubleshooting

**"Ollama not available"**
- Open Task Manager â†’ Check if "Ollama" is running
- If not, search for "Ollama" in Start menu and launch it

**"Model not found"**
```powershell
ollama list  # Check installed models
ollama pull llama3.2:1b  # Install if missing
```

**LLM is slow**
- First run is always slower (model loading)
- Subsequent requests are faster (~200-300ms)
- Use smaller model if needed (llama3.2:1b is already the smallest)

## What You Get

âœ… Better handling of OCR errors (VERY â†’ EVERY, 20N@) â†’ 20MG)
âœ… More accurate patient name extraction
âœ… Drug names extracted even with trailing garbage
âœ… 100% local - no internet needed
âœ… No API costs
âœ… Privacy-friendly - data never leaves your device

Need help? Check `OLLAMA_SETUP.md` for detailed documentation.
